diff --git a/common_utils/py/multi_counter.py b/common_utils/py/multi_counter.py
index 0e5c1a9..519ab79 100644
--- a/common_utils/py/multi_counter.py
+++ b/common_utils/py/multi_counter.py
@@ -69,6 +69,8 @@ class MultiCounter:
         wb_run_name=None,
         wb_group_name=None,
         config=None,
+        wb_project=None,
+        wb_entity=None,
     ):
         self.stats = defaultdict(lambda: ValueStats())
         self.last_time = datetime.now()
@@ -79,13 +81,15 @@ class MultiCounter:
         self.use_wandb = use_wandb
         if use_wandb:
             os.environ["WANDB_INIT_TIMEOUT"] = "300"
-            wandb.init(
-                # set the wandb project where this run will be logged
-                project=wb_exp_name,
+            init_kwargs = dict(
+                project=wb_project if wb_project else wb_exp_name,
                 name=wb_run_name,
                 group=wb_group_name,
                 config={} if config is None else config,
             )
+            if wb_entity:
+                init_kwargs["entity"] = wb_entity
+            wandb.init(**init_kwargs)
 
     def __getitem__(self, key):
         if len(key) > self.max_key_len:
diff --git a/release/cfgs/robomimic_rl/square_state_rlpd_online.yaml b/release/cfgs/robomimic_rl/square_state_rlpd_online.yaml
new file mode 100644
index 0000000..7b2fad3
--- /dev/null
+++ b/release/cfgs/robomimic_rl/square_state_rlpd_online.yaml
@@ -0,0 +1,27 @@
+task_name: "NutAssemblySquare"
+episode_length: 300
+stddev_max: 0.1
+use_state: 1
+state_stack: 3
+mix_rl_rate: 0.5
+preload_num_data: 50
+preload_datapath: "release/data/robomimic/square/processed_data96.hdf5"
+num_warm_up_episode: 50
+num_train_step: 10000000
+replay_buffer_size: 1000
+use_wb: 1
+wandb_project: "square-online-rl-saturation"
+run_name_method: "rlpd"
+mp_eval: 1
+log_per_step: 100000
+num_eval_episode: 100
+num_critic_update: 5
+q_agent:
+  act_method: "rl"
+  state_critic:
+    num_q: 5
+    layer_norm: 1
+    hidden_dim: 1024
+  state_actor:
+    hidden_dim: 1024
+    dropout: 0
diff --git a/scripts/square_online_rl.sbatch b/scripts/square_online_rl.sbatch
new file mode 100644
index 0000000..309cc07
--- /dev/null
+++ b/scripts/square_online_rl.sbatch
@@ -0,0 +1,22 @@
+#!/bin/bash
+#SBATCH --time=4-00:00:00
+#SBATCH --cpus-per-task=12
+#SBATCH --mem=32GB
+#SBATCH --gres=gpu:1
+#SBATCH --job-name=rlpd_square
+#SBATCH --output=logs/rlpd_square_%j.out
+
+# Create logs directory if it doesn't exist
+mkdir -p logs
+
+# Set up environment
+export MUJOCO_GL=egl
+
+# Navigate to IBRL directory
+cd /iris/u/ankile/self-improving-robots-workspace/ibrl
+
+# Use the IBRL conda environment
+PYTHON=/iris/u/ankile/envs/ibrl/bin/python
+
+# Run training (seed can be overridden with --seed)
+$PYTHON train_rl.py --config_path release/cfgs/robomimic_rl/square_state_rlpd_online.yaml ${@}
diff --git a/train_rl.py b/train_rl.py
index 2b553e9..675659a 100644
--- a/train_rl.py
+++ b/train_rl.py
@@ -67,6 +67,26 @@ class MainConfig(common_utils.RunConfig):
     # log
     save_dir: str = "exps/rl/run1"
     use_wb: int = 0
+    wandb_project: str = "ibrl"
+    wandb_entity: str = ""  # empty = use default
+    run_name_method: str = ""
+
+    @property
+    def wb_run(self):
+        """Generate EXPO-style run name: {method}_{env_name}_s{seed}"""
+        if not self.use_wb:
+            return None
+        # Allow explicit run labeling for baselines (e.g., rlpd) without changing behavior.
+        method = self.run_name_method if self.run_name_method else self.q_agent.act_method
+        # Extract env name from task_name (e.g., "NutAssemblySquare" -> "square")
+        env_name = self.task_name.lower()
+        if "square" in env_name:
+            env_name = "square"
+        elif "can" in env_name:
+            env_name = "can"
+        elif "lift" in env_name:
+            env_name = "lift"
+        return f"{method}_{env_name}_s{self.seed}"
 
     def __post_init__(self):
         self.rl_cameras = self.rl_camera.split("+")
@@ -251,11 +271,11 @@ class Workspace:
             assert self.cfg.save_per_success <= 0, "cannot save a non-growing replay"
             self.replay.freeze_bc_replay = True
 
-    def eval(self, seed, policy) -> float:
+    def eval(self, seed, policy) -> dict:
         random_state = np.random.get_state()
 
         if self.cfg.mp_eval:
-            scores: list[float] = run_eval_mp(
+            eval_result: dict = run_eval_mp(
                 env_params=self.eval_env_params,
                 agent=policy,
                 num_proc=10,
@@ -264,7 +284,7 @@ class Workspace:
                 verbose=False,
             )
         else:
-            scores: list[float] = run_eval(
+            eval_result: dict = run_eval(
                 env_params=self.eval_env_params,
                 agent=policy,
                 num_game=self.cfg.num_eval_episode,
@@ -274,7 +294,7 @@ class Workspace:
             )
 
         np.random.set_state(random_state)
-        return float(np.mean(scores))  # type: ignore
+        return eval_result
 
     def warm_up(self):
         # warm up stage, fill the replay with some episodes
@@ -320,7 +340,10 @@ class Workspace:
             wb_run_name=self.cfg.wb_run,
             wb_group_name=self.cfg.wb_group,
             config=self.cfg_dict,
+            wb_project=self.cfg.wandb_project,
+            wb_entity=self.cfg.wandb_entity if self.cfg.wandb_entity else None,
         )
+        self.stat = stat
         self.agent.set_stats(stat)
         saver = common_utils.TopkSaver(save_dir=self.work_dir, topk=1)
 
@@ -353,6 +376,10 @@ class Workspace:
                     stat["score/train_score"].append(float(success))
                     stat["data/episode_len"].append(self.train_env.time_step)
 
+                    # Log EXPO-aligned training metrics
+                    stat["train/return"].append(self.train_env.episode_reward)
+                    stat["train/length"].append(self.train_env.time_step)
+
                     # reset env
                     obs, _ = self.train_env.reset()
                     self.replay.new_episode(obs)
@@ -388,8 +415,16 @@ class Workspace:
         with stopwatch.time("eval"):
             eval_seed = (self.global_step // self.cfg.log_per_step) * self.cfg.num_eval_episode
             stat["eval/seed"].append(eval_seed)
-            eval_score = self.eval(seed=eval_seed, policy=self.agent)
-            stat["score/score"].append(eval_score)
+            eval_result = self.eval(seed=eval_seed, policy=self.agent)
+
+            # Log new EXPO-aligned metrics
+            stat["eval/success_rate"].append(eval_result["success_rate"])
+            stat["eval/success_length_mean"].append(eval_result["success_length_mean"])
+            stat["eval/return"].append(eval_result["return_mean"])
+            stat["eval/length"].append(eval_result["length_mean"])
+
+            # Keep old metric for backward compat
+            stat["score/score"].append(eval_result["success_rate"])
 
             original_act_method = self.agent.cfg.act_method
             # if self.agent.cfg.act_method != "rl":
@@ -400,12 +435,12 @@ class Workspace:
 
             if self.agent.cfg.act_method == "ibrl_soft":
                 with self.agent.override_act_method("ibrl"):
-                    greedy_score = self.eval(seed=eval_seed, policy=self.agent)
-                    stat["score/greedy_score"].append(greedy_score)
-                    stat["score_diff/greedy-soft"].append(greedy_score - eval_score)
+                    greedy_result = self.eval(seed=eval_seed, policy=self.agent)
+                    stat["score/greedy_score"].append(greedy_result["success_rate"])
+                    stat["score_diff/greedy-soft"].append(greedy_result["success_rate"] - eval_result["success_rate"])
             assert self.agent.cfg.act_method == original_act_method
 
-        saved = saver.save(self.agent.state_dict(), eval_score, save_latest=True)
+        saved = saver.save(self.agent.state_dict(), eval_result["success_rate"], save_latest=True)
         stat.summary(self.global_step, reset=True)
         print(f"saved?: {saved}")
         stopwatch.summary(reset=True)
@@ -442,6 +477,8 @@ class Workspace:
             wb_run_name=self.cfg.wb_run,
             wb_group_name=self.cfg.wb_group,
             config=self.cfg_dict,
+            wb_project=self.cfg.wandb_project,
+            wb_entity=self.cfg.wandb_entity if self.cfg.wandb_entity else None,
         )
         saver = common_utils.TopkSaver(save_dir=self.work_dir, topk=1)
 
